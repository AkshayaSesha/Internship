# -*- coding: utf-8 -*-
"""User Question Answering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13PI4ZI1Gw8AxVpS7ZDKj2bNFb0Tr0I_b
"""

pip install transformers datasets torch

from transformers import BertTokenizerFast, BertForQuestionAnswering, AdamW
from datasets import load_dataset
import torch

tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')
model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')

squad = load_dataset("squad")

def preprocess_function(examples):
    questions = [q.strip() for q in examples['question']]
    inputs = tokenizer(
        questions,
        examples['context'],
        max_length=384,
        truncation=True,
        return_offsets_mapping=True,
        padding="max_length",
        return_tensors="pt"
    )
    start_positions = []
    end_positions = []

    for i, offset_mapping in enumerate(inputs["offset_mapping"]):
        answer = examples["answers"][i]["text"][0]
        start_char = examples["answers"][i]["answer_start"][0]
        end_char = start_char + len(answer)
        sequence_ids = inputs.sequence_ids(i)
        context_start = sequence_ids.index(1)
        context_end = len(sequence_ids) - 1 - sequence_ids[::-1].index(1)

        start_position = context_start
        end_position = context_end

        for idx, mapping in enumerate(offset_mapping):
            if mapping[0] <= start_char < mapping[1]:
                start_position = idx
            if mapping[0] < end_char <= mapping[1]:
                end_position = idx
                break

        start_positions.append(start_position)
        end_positions.append(end_position)

    inputs["start_positions"] = torch.tensor(start_positions)
    inputs["end_positions"] = torch.tensor(end_positions)
    return inputs
tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad["train"].column_names)

from torch.utils.data import DataLoader
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

train_dataset = tokenized_squad["train"].select(range(1000))
val_dataset = tokenized_squad["validation"].select(range(100))

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=data_collator)
val_loader = DataLoader(val_dataset, batch_size=8, collate_fn=data_collator)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

optimizer = AdamW(model.parameters(), lr=5e-5)

# Training loop
for epoch in range(3):
    model.train()
    for batch in train_loader:
        optimizer.zero_grad()
        inputs = {k: v.to(device) for k, v in batch.items() if k != "offset_mapping"}

        if "start_positions" not in inputs or "end_positions" not in inputs:
            continue

        outputs = model(**inputs)
        loss = outputs.loss

        loss.backward()
        optimizer.step()

        print(f"Loss: {loss.item()}")

def answer_question(question, context):
    inputs = tokenizer(question, context, return_tensors='pt', max_length=384, truncation=True)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model(**inputs)
    answer_start = torch.argmax(outputs.start_logits)
    answer_end = torch.argmax(outputs.end_logits)

    answer = tokenizer.decode(inputs['input_ids'][0][answer_start:answer_end+1], skip_special_tokens=True)
    return answer

context = input("Enter passage:")
question = input("Enter question:")

print(answer_question(question, context))